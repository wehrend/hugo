[{"id":0,"href":"/posts/web/04_odoo_hacks/","title":"Odoo hacks: Remove Lock-Button from Odoo v12","section":"Web","content":"Today I want to introduce a solution for a usability-problem I had in Odoo recently. In Odoo 12 there is a lock button next to the edit / save button, and you have to click both to (1) unlock the page and (2) make it editable. This should be done in one step, effectively removing the locking mechanism used in the Delivery Orders (class StockPicking) and Manufacturing Orders (class MrpProduction). The solution requires an xml and a javascript-file as well as a dedicated web-controller. The soliution is presented below.\n image:../odoo_hacks.png [Odoo Hacks]\n /static/src/xml/trigger_toggle.xml \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;openerp\u0026gt; \u0026lt;data\u0026gt; \u0026lt;template id=\u0026#34;assets_backend\u0026#34; name=\u0026#34;trigger_toggle assets\u0026#34; inherit_id=\u0026#34;web.assets_backend\u0026#34;\u0026gt; \u0026lt;xpath expr=\u0026#34;.\u0026#34; position=\u0026#34;inside\u0026#34;\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;/nm_data_stock/static/src/js/trigger_toggle.js\u0026#34;/\u0026gt; \u0026lt;/xpath\u0026gt; \u0026lt;/template\u0026gt; \u0026lt;/data\u0026gt; \u0026lt;/openerp\u0026gt;   The code is pretty self-explanatory. We use the xml file to add the script. Dont forget to add it in the manifest-file. The javascript functions _onEdit and _onSave are functions from the core webclient (to be more precise web.FormController), and we add some simple url-parsing and an ajax-RPC-call to both functions.\n static/src/js/trigger_toggle.js odoo.define(\u0026#39;nm_data_stock.trigger_toggle\u0026#39;, function(require){ \u0026#39;use strict\u0026#39;; var ajax = require(\u0026#39;web.ajax\u0026#39;) var FormController = require(\u0026#39;web.FormController\u0026#39;); var triggerButton = FormController.include({ _onEdit: function () { // wait for potential pending changes to be saved (done with widgets // allowing to edit in readonly) this.mutex.getUnlockedDef().then(this._setMode.bind(this, \u0026#39;edit\u0026#39;)); const url = new URL(this.$el.context.baseURI) const parsedHash = new URLSearchParams(url.hash.substring(1)) const id = parsedHash.get(\u0026#39;id\u0026#39;) const model = parsedHash.get(\u0026#39;model\u0026#39;) if (model.toString() == \u0026#39;mrp.production\u0026#39; || model.toString() == \u0026#39;stock.picking\u0026#39;) { console.log(\u0026#39;OnEdit \u0026#39;+ id + \u0026#39; \u0026#39;+ model); ajax.jsonRpc(\u0026#39;/web/webclient/trigger_toggle\u0026#39;, \u0026#39;call\u0026#39;, {\u0026#39;id\u0026#39;: id,\u0026#39;model\u0026#39;: model,\u0026#39;button_state\u0026#39;: \u0026#39;edit\u0026#39;} ) } }, _onSave: function (ev) { ev.stopPropagation(); // Prevent x2m lines to be auto-saved var self = this; this._disableButtons(); const url = new URL(this.$el.context.baseURI) const parsedHash = new URLSearchParams(url.hash.substring(1)) const id = parsedHash.get(\u0026#39;id\u0026#39;) const model = parsedHash.get(\u0026#39;model\u0026#39;) if (model.toString() == \u0026#39;mrp.production\u0026#39; || model.toString() == \u0026#39;stock.picking\u0026#39;) { console.log(\u0026#39;OnSave \u0026#39;+ id + \u0026#39; \u0026#39;+ model); ajax.jsonRpc(\u0026#39;/web/webclient/trigger_toggle\u0026#39;, \u0026#39;call\u0026#39;, {\u0026#39;id\u0026#39;: id, \u0026#39;model\u0026#39;: model,\u0026#39;button_state\u0026#39;: \u0026#39;save\u0026#39;} ) } this.saveRecord().always(function () { self._enableButtons(); }); }, }) })   Below is shown the webcontroller as endpoint for the ajax-RPC-call shown above…​\n controllers/main.py\n from odoo import http from odoo import api,fields from urllib import parse class ToggleController(http.Controller): @http.route(\u0026#39;/web/webclient/trigger_toggle\u0026#39;, type=\u0026#39;json\u0026#39;, auth=\u0026#34;none\u0026#34;) def trigger_toggle(self, **kw): id = kw.get(\u0026#39;id\u0026#39;) model = kw.get(\u0026#39;model\u0026#39;) if model and id: print(model) obj = http.request.env[model].browse(int(id)) return obj.sudo().trigger_toggle(kw.get(\u0026#39;button_state\u0026#39;))   Last but not least the implementation in the both classes.\n models/stock.py\n class StockPicking(models.Model): \u0026#34;\u0026#34;\u0026#34;Inherit class StockPicking from module stock.\u0026#34;\u0026#34;\u0026#34; def trigger_toggle(self, state): if state==\u0026#39;edit\u0026#39;: self.is_locked = False if state==\u0026#39;save\u0026#39;: self.is_locked = True class MrpProduction(models.Model): \u0026#34;\u0026#34;\u0026#34;Inherit class MrpProduction\u0026#34;\u0026#34;\u0026#34; def trigger_toggle(self, state): if state==\u0026#39;edit\u0026#39;: self.is_locked = False if state==\u0026#39;save\u0026#39;: self.is_locked = True   "},{"id":1,"href":"/docs/digital_logic_x/x2_cordic/","title":"X2_cordic","section":"Digital Logic X","content":"The CORDIC algoritm and direct digital synthesis (DDS) The CORDIC (which stands for \u0026#34;coordinate rotation digital computer\u0026#34;) algorithm, was developed in 1956 by Jack.E. Volder, to replace the analog resolvers used for missiles navigation by digital computation on digital computer. The algorithm turned out to be very successfull, today it can be found in every pocket calculator, doing the computation of the trigonometrical functions sinus, cosinus and tangens. But the algorithm is not limited to that it can also calculate logarithmic and exponential functions, given the needed modifications.\n \n   The CORDIC algorithm In the image below, V0 shows the start vector\n \\[v_{0}=\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\]\n which we now iterate, by multiplying it with the rotation matrix, given below:\n \\[v_{i+1}= R_i v_i\\]\n \\[ R_i = \\begin{bmatrix} cos(\\theta) \u0026amp; -sin(\\theta) \\\\ sin(\\theta) \u0026amp; cos(\\theta) \\end{bmatrix} \\]\n \n \n   "},{"id":2,"href":"/posts/web/03_separate_form_and_content/","title":"Separating blog and content","section":"Web","content":"As the blog grow bigger and after the move to odoo, I noticed a gap in the workflow,since a blog and its content itself are different entities, I wanted to handle them as such, that led me to a cleanup of the original source code for blog, where I moved teh content folder into a different repo, called hugo_content, the former repo I called hugo_blog. ---\n In my case, this means you have to clone the repo hugo_blog first go inside the new directory (cd hugo ), checkout its branch \u0026#39;hugo\u0026#39; and then do a clone of the repo hugo_content.\n "},{"id":3,"href":"/docs/digital_logic_x/x0_multiplication/","title":"X0_multiplication","section":"Digital Logic X","content":"Multiplication and Divsion Multiplication operations can be implemented in very different ways: Slow, as a serial operation, controlled by a micro program or fast in dedicated hardware.\n The division is a more complex operation, and thus will be discussed in another post. Also for multiplication we set the scope to unsigned integer numbers.\n Shift left and shift right For multipliers that are powers of two, the operation is very simple and can be implemented via a shift left.\n A shift left by one bit equals a multiplication by 2, as left shift by n bits equals a multiplication by 2^n.\n A shift right is a division by 2, a right shift by n bits is a division by 2^n\n \n  The multiplication algorithm Of course we are not only interested in the special case of base two multipliers, but rather want the multiplication operation to work on all numbers we defined above. We begin with a serial multiplication implementation.\n To start with, we analyze the steps of a binary multiplication, which frankly does not differ much from a multiplication in the decimal system and is even simpler. We multiply 7 times 6 in the binary system which results in 42.\n multiplicand x multiplier = product\n \\[ \\begin{aligned} 0111_2 \\times 0110_2 \\\\ \\hline 0000 \\\\ 011110 \\\\ 011100 \\\\ 0111000 \\\\ \\hline 0101010 \\\\ \\end{aligned} \\]\n As we can see, based on that example, the multiplication operation is a sequence of shifting and addition. From this, we can now dissect the multiplication operation and derive an algorithm from it.\n For details we refer to the original source: Rechnerentwurf: Rechenwerke, Mikroprogrammierung, RISC by R. Hoffman, third edition, Oldenbourg Verlag.\n We have a multiplicator of X[n] and a multiplier of Y[m], then the product of the multiplication operation results in P[n+m], meaning the size of the operation, logically, is the addition of both the size of the multiplicator and the multiplier.\n  Booth Algorithm The booth algorithm is one of the most efficient algorithm, as said above, it is again a series of shifting and addition. But here the three following rules have to be considered (see here for reference ): 1. The multiplicand is subtracted from the partial product upon encountering the first least significant 1 in a string of 1’s in the multiplier 2. The multiplicand is added to the partial product upon encountering the first 0 (provided that there was a previous ‘1’) in a string of 0’s in the multiplier. 3. The partial product does not change when the multiplier bit is identical to the previous multiplier bit.\n The booth algorithm applied can be watched here: \n Booth hardware implementation and flowchart The booth multiplier, shown below, consist of the registers A,B for the multiplier and multiplicand and Q for the result. The register AC is the accumulator,the bit register BR and the register QR. An extra flipflop Qn+1 is used to check the multiplier.The flowchart is given below. \n \n Initially the accumulator and the flipflop Qn+1 are cleared, reset to zero.The sequence counter SC ist set to the number of bits n of the multiplier. Then the two bits in Qn and Qn+1 are checked. In case these are 10 the multiplicand gets subtracted from the partial product residing in the Accumulator AC. In case they are 01 the multiplicand gets additioned to the partial product residing in the Accumulator AC. When the two bit are the same (00,11) the partial product is unchanged. Since the subtraction and addition alternate, an overflow cannot occur. As next step the partial product and the multiplier (plus Qn+1) are shifted right. This is an arithmetic shift (ashr) which shifts AC and QR to the right, thus the sign bit in AC is unchanged. The sequence counter is decremented and the computational loop gets repeated n times. When multiplying negative numbers we need to find 2’s complement because it is easier to add instead of doing a binary subtraction.\n     "},{"id":4,"href":"/docs/digital_logic/00_combinatorial_logic/","title":"00_combinatorial_logic","section":"Digital Logic","content":"This course about digital logic is splitted into two big chunks:\n  Combinatorial circuits (without memory elements)\n  Sequential circuits (with memory elements)\n   \n While combinatorial circuits consists of boolean gates interconnected without feedback function, sequential gates posses a memory function,which makes them able to progress to another state.\n A combinatorial circuit can have n -wide vector of inputs and an m -wide vector of outputs.\n Combinatorial circuits can be described in different ways: As a formula, a circuit, a truth table or a Karnaugh map. All these descriptions are convertible into each other.\n \n So lets get started with the fundamental logic gates and boolean algebra in the next post.\n "},{"id":5,"href":"/posts/web/02_moving-to-hugo/","title":"Moving to Hugo","section":"Web","content":"Since one week my blog is build on the static-site-generator Hugo. I wanted to move to Hugo far earlier, due to the wide variety of themes. However this required the move from Asciidoc to Asciidoctor, and I experienced a few problems with the integration of Asciidoctor into my existing workflow. As the approach of rendering the latex-equations in the asciidoc-documents no loger worked, I decided to move on to client-based rendering of formulas, like done in KaTex and Mathjax.\n So to install Hugo, Asciidoctor and its dependencies without headaches, we again make use of Docker and its huge ecosystem: Sombody already created Docker images for hugo with asciidoctor integrated, so we do not need to go thru the sometimes cumbersome setup of Asciidoctor and its dependencies.\n With docker installed, in the shell (for Linux and Mac OS, for Windows the powershell) we do a\n docker pull klakegg/hugo:edge-asciidoctor   which pulls an image with the latest Hugo version and Asciidoctor integrated from the Docker-repo klakegg .\n We navigate to the parent-directory of our blog, and run the following command:\n docker run -it -v $(pwd):/src klakegg/hugo:edge-asciidoctor new site hugo/blog   This creates the skeleton of our new site, looking like that:\n old-blog hugo └── blog ├── archetypes ├── config.toml ├── content ├── data ├── layouts ├── static └── themes   We have no theme installed yet, so go to themes and select your favorite, mine is Book. We install it via simple download or via git\n cd hugo/themes git clone https://github.com/alex-shpak/hugo-book   and add the line\n theme = \u0026#34;hugo-book\u0026#34;   to our config.toml. While we are at it we also add the following policy, necessary since hugo version 0.91, to our config.\n  enableInlineShortcodes = false [security.exec] allow = [\u0026#39;^dart-sass-embedded$\u0026#39;, \u0026#39;^go$\u0026#39;, \u0026#39;^npx$\u0026#39;, \u0026#39;^postcss$\u0026#39;, \u0026#39;^asciidoctor$\u0026#39;] osEnv = [\u0026#39;(?i)^(PATH|PATHEXT|APPDATA|TMP|TEMP|TERM)$\u0026#39;]     Now we need to move the content from the old blog to new, we do something similar to this\n cp ../old_blog/pages ../hugo/content/docs cp ../old_blog/posts ../hugo/content/posts cp ../old_blog/images ../hugo/content/docs/images   Now we need to convert the adoc-headers to the hugo frontmatter format,and also adapt the asciidoc syntax to the new. To make our new blog digesting the equations and formulas written in Latex, we need to add the script for KaTex / Mathjax at some layout-file in the theme of our choice: Integrate KaTex\n To see what we need to change we execue the hugo-server and in the browser head over to the given address http://localhost:1313\n docker run -it -v $(pwd):/src -p 1313:1313 swehrend/hugo-ext-asciidoctor:edge-ext-debian server --renderToDisk   which results in something similar to this:\n \n If we are satisfied with the result we can deploy our site with an rsync to our webspace, something similar to the following command:\n rsync -rav public/ wehrend@giclas.uberspace.de:/var/www/virtual/wehrend/html   \n "},{"id":6,"href":"/docs/digital_logic/07_risc_v/","title":"07_RISC-V","section":"Digital Logic","content":"Digital design meets computer architecture   RISC vs CISC and The RISC-V architecture Existing processor types can be classified by their instruction type set: Reduced instruction set computer (RISC), Complex instruction set computer (CISC) and hybrid forms. While mini processors like ARM for embedded systems mostly utilise reduced instruction sets,todays workstation and Server Architectures (x86, x86_64) are hybrids of RISC and CISC. The idea of a reduced instruction set is to avoid big complex instructions and multiple different addressing modes (as those typically used in x86 architectures). Advantages of a RISC architecture are shorter pipelines which allows faster clocking. RISC architectures follow a simple scheme: \u0026#39;Fetch → Decode → Fetch Operands → Execute → Write Back\u0026#39; Also instructions have a constant length, memory operations are divided from arithmetic operations, which is known as Load/Store-architecture. RISC-V is an open standard for the instruction set archtitecture (ISA). Most of this post is referenced from the popular book Computer Organization and Design (RISC-V) Edition by Hennessey and Patterson.\n RISC-V Assembler Here we introduce the RISC-V assembler. The subset shown here is a bit bigger than the one we will implement further down the line. For example also immediate instructions like \u0026#39;addi\u0026#39; are shown, necessay as a way to load constants into registers. The example program shown below, counts from 0 to 10, utilizing a loop.\n ADDI x2, x0, 1 ADDI x3, x0, 10 loop: ADD x1, x1, x2 SW x1, 4(x0) BNE x3, x1, loop HLT   Register-type: Instruction [dest. reg] [reg1] [reg2]\n Load / Store-type: Instruction [dest. reg] [byte offset(reg1)]\n Branch-type: Instruction [dest. reg] [reg1] [dest. label]\n For the load / store instructions the reg1 contains the start address while the byte offset contains the size of the value loaded into the register (normaly 4) / respectively written to the memory.\n An overview of the instruction formats of the different types is given below:\n \n   A minimal Implementation The minimal working subset of a RISC-V implementation contains the following instructions:\n   Arithmetic-logical instructions \u0026#39;add\u0026#39;, \u0026#39;sub\u0026#39;, \u0026#39;and\u0026#39; and \u0026#39;or\u0026#39; [Register-type instructions]\n  Memory reference instructions like load word (lw) and store word (sw) [Store-type instructions]\n  Conditional branch instruction(s) like branch-if-equal (beq) [Branch-type instructions]\n   We will see how the instruction set architecture choosen, affects performance-related key aspects like clock rate and Cycles-per-instruction (CPI). We will also see, that the different instruction types only differ in the later stages of the Fetch→Decode→ Execute-cycle\n For every instruction the first two steps are identical:\n  The program counter (PC) points to the current code that will be executed.The address is sent to the memory to fetch the current instruction from the memory.\n  Read two (one for the lw-instruction) registers, the instruction fields contains the register numbers.\n   For the next steps the actions depend on the instruction type, but are still utilizing / sharing the same resources. For example the ALU is used by Register-type instruction to compute data, while it is also used by Store-type instructions to compute the address, and the Branch-instructions for the equality test. Only after that ALU, the different instruction types really differ.\n \n As we will see the abstract schematic shown below does not explain all cases. We need another schematic extended, showing also the control parts.\n \n   The Fetch-Decode-Execute cycle for the different instruction types We will start with the common Fetch-Decode-Execute cycle of a common RISC processor, and demonstrate the working principle of it for typical instructions.\n The full cycle is: Fetch → Decode → Fetch Operands → Execute → Write Back\n     Type example instruction Fetch Decode Fetch Operands Execute Write Back     Register\n add, sub,and,or\n fetch instruction, increase PC by 4\n Decode instruction\n Fetch operands from registers\n Execute calculation in ALU\n Write value back to data memory.\n   Store\n lw,sw\n fetch instruction, increase PC by 4\n Decode instruction\n operand\n Calculate address\n read / write data from / to memory\n   Branch\n beq\n fetch instruction,set PC to destination address\n Decode instruction\n Fetch operands from registers\n Test for equality (for beq)\n switch multiplexer for address calculation to second adder\n    The instruction part To explain a cpu on this abstract level, we need to introduce some more concepts, e.g that an instruction, stored in memory, is accessible under a specific address. An address in RISC-V standards is a 32 bit long value, pointing to a certain cell in a memory array. The program counter (PC) is a register which points to a certain address in the memory / register file. The program counter is connected to the first address-adder with a contant intger of 4 (the adress offset, 4 * 1 byte = 32 bit).\n \n If the instruction in the memory is an address modifying instruction, the given value will be added by the second address-adder. This way we can generate jump instructions.\n \n  The data path The data path shows some elements we already are familiar with - the ALU - as well as elments we are not yet familiar with - the memory blocks - on the right the so-called register file and on the left the data memory. Both inputs of the ALU are connected to one (different) register.\n \n  The control part The control part is the most black-boxed element we see. We know already how the multiplexers work, and we see already a feedback line for the branch control. But most of the elements in this abstract view remain unknown to the reader. Let us change this by having a closer look into it.\n \n    "},{"id":7,"href":"/docs/digital_logic/06_memory/","title":"06_Memory","section":"Digital Logic","content":"Memory Organization In this blog post the author will introduce the different types of semiconductor memory. This is a wide field, so the goal here is to focus on the most important ones and draw the differences between the different types.\n The Memory hierarchy As in computing all is about performance, we first have a look at the memory hierarchy. As you can see in the pyramid below, memory is ordered in layers. The slowest elements -also the ones with highest latency - are at the bottom of this pyramid, while the fastest ones are shown at the apex. Imagine your desk is the actual CPU, with an open book laying in front of your eyes. This is the register. Now you read in this book a reference to another book in your bookshelf. Your bookshelf is the cache here. The cache itself is again layered. Lets say L1-cache is the bookshelf in your room, L2-cache the bookshelf in another room and L3-cache books stored in a box in the same room. If you now have a reference in your current book (the one on your desk), that refers to a book you do not possess, you need to go to the public library. The public library is your RAM. The access times are really high. And lets say the book you need is also not available there, and they must order it from a library in another city. Then you got to the level at the bottom, which is the disk storage unit.\n \n At the bottom is also the low-priced memory (per byte). The higher you go on the hierarchy the costlier the memory becomes. In this article we will expand our knowledge about the RAM. Caches are more complex and will eventually be handeled in a future article.\n  RAM and ROM The image below shows the overall organization of a ram circuit, RAM stand for random access memory, so it means a memory with arbitrary access. Simply put, you set an address in and get the data which is stored under the given address location out.The schematic given here also holds for Read only memory (ROM). Complementary to ROM, which is persistent, RAM is volatile. So it needs ongoing power supply,otherwise it loses its stored data.\n \n As can be seen the main parts apart from the memory itself are column- and row-decoder, as well as read / write amplifier. The address-decoder just consists of well-known combinatorial logic, while the amplifier are a bit more complex. The blue dots on the intersection of rows and columns mark the placements of the memory cell, depending on the type of memory, those differ and are explained in the next section.\n The different memory cells     static RAM (SRAM)\n dynamic RAM (DRAM)\n   \n \n   transistor # 4\n transistor # 1\n   \n    transistor # 6\n       SRAM vs. DRAM The static RAM (SRAM) cell has the advantage of holding the stored value as long as the power supply is not interrupted. Its clear disadvantage is the circuit effort of minum 4 transistor (for an nmos design) but regularly 6 for a cmos design.That makes it ideal for small pockets of memories like registers and cache located near the cpu. Complementary the dynamic RAM (DRAM) cell needs just one transistor and one capacitor to hold the stored value, but needs to be refreshed periodically.\n  The register file \n At this point the author wants to introduce the so-called register file. This is a type of memory which can have multiple read ports. which is useful as input for the ALU we introduced in the last blog post. While the amount of read ports is theoretically unlimited, the amount of write ports is usually one. The reason, as the vivid reader can imagine is to mitigate hazards, which is with multiple read ports difficult to handle.\n \n \n    "},{"id":8,"href":"/docs/digital_logic_2/11_clocks_and_registers/","title":"11_Clocks_flipflops_and_registers","section":"Digital Logic 2","content":"Clocks, Flipflops and registers In this blog post the author will introduce multiple elements needed at a later stage.\n Register-Transfer-Level needs a clock source As for now we only learned about combinatiorial logic, which works without any clock source - every signal is just delayed by the time it needs to travel through the circuit. This is known as asynchronous. But for more sophisticated tasks, sequential circuits, having registers storing the input and output signals are necessary to get correct results. This is known as synchronous. Most, if not all designs used nowadays are synchronous, due to a lower design complexity.\n Register-Transfer-Level\n \n The blue parts in the image above are showing registers where the values are stored, we will see later how those are implemented, lets first have a look at the clock source which is needed.\n There are different ways to create a clock source:\n  A crystal oscillator\n  A ring oscillator\n   The former one is the classical, the later one a more modern variant.\n     crystal oscillator\n ring oscillator\n   \n \n   typical frequence range: 1Mhz..100Mhz\n typical frequence range: 1Hz..15Mhz(discrete layout)\n   \n \n   frequency determined by crystal geometry\n frequency determined by propagation delay and number of gates\n   accuracy mostly dependent from temperature; for precise applications crystal ovens are used\n accuracy mostly dependent from power supply stability\n    The crystal oscillator The crystal oscilltor is a clock source based on a quartz crystal. The frequency is dependent on the geometry of the quartz. Crystal oscillators are available in a metal box housing, with additional wiring, with the quartz being the heart of the circuit. The circuit is commonly based on the Pierce oscillator.\n \n Their accuracy is mostly dependent on the temperature,so for applications with high accuracy needs, they are housed in a quartz oven, holding the quartz on a constant temperature. An interesting teardown of such an oscillator is available on Ken Shirriffs blog.\n  The ring oscillator Another common source of a clock source is the ring oscillator. The trick here is to chain an odd number of inverter gates and feedback the output of the last inverter as input to the first inverter. This way we create an oscillating circuit.\n \n The frequency is dependent on the number of inverters as well as the propagation time Tp (see the equation below). The time the signal needs to travel trough the gate is known as propagation time Tp.\n \\[f_{ro} = \\frac{1}{2nT_{p}}\\]\n Here the accuracy is mostly dependent on the stability of the power supply. An example where it is used - e.g. the 8087, intels® Floating Point Unit - is again provided by Ken Shirriffs blog. As shown there, the frequency can be decreased by adding an RC-network between the inverters.\n  The classic RS-flipflop The classic flipflop consists of two Nand gates which outputs are feedback’ed to the complementary gate, as can be seen in the table below. Below the circuit the truth table is given. As can be seen the flipflop locks the output to one state, \u0026#39;1\u0026#39; or \u0026#39;0\u0026#39;. If both inputs are set low, the output is undefined (and conforms to the preferred position).\n     RS-flipflop structure\n RS-flipflop symbol\n   \n \n        ̅R\n ̅S\n  Q\n ̅Q\n comments\n   0\n 0\n  1\n 1\n invalid\n   0\n 1\n  1\n 0\n Reset\n   1\n 0\n  0\n 1\n Set\n   1\n 1\n  Q\n ̅Q\n —     One application of the classic RS-flipflop is to debounce switches.\n But for most applications a more evolved flipflop is necessary. Let’s have a look at the (D)ata-flipflop.\n  The D-flipflop In the D-flipflop the circuit of the RS-flipflop is preceeded by two Nand gates, controlled by a clock impulse. Only when the clock is high (i.e. \u0026#39;1\u0026#39;) the date applied to the (D)ata-input is valid and stored in the RS-fliplop. Otherwise the applied data input is invalid.\n     D-flipflop structure\n D-flipflop symbol\n   \n \n        Clock\n D\n Q\n ̅Q\n comments\n   🠓 \u0026gt;\u0026gt;0\n x\n Q\n ̅Q\n no change\n   🠓 \u0026gt;\u0026gt;0\n 1\n 1\n 0\n Reset\n   🠑 \u0026gt;\u0026gt;1\n 0\n 0\n 1\n Set\n   🠑 \u0026gt;\u0026gt;1\n 1\n Q\n ̅Q\n -\n      A binary counter Using the D-flipflop we got acquainted with in the last section, we can construct an (asynchronous) binary counter. This output can be used as adresses for a ROM table, as we will see in another blog post.\n     Asynchronous binary counter with D-flipflops\n   \n    Using the outputs independent, it can be seen that the counter also works as frequency divider: On every output the frequency is divided by two referenced to the previous one.\n    Registers What are registers? Registers are used to hold respectively store values. Every slighlty more complex nowadays CPU has copious quantities of registers inside. Register sizes vary widely dependent on the application. Reaching from status registers only holding one bit to registers over 32 and 64 bit for regular registers until vector registers with a size of 64 bytes (= 512 bits). Those registers are named after a certain scheme, in some architectures with numbers (e.g. MIPS and ARM), other times with a more comnplex scheme of alphabetic numbers (like in x86 architectures).\n \n In the previous sections we have learned about the flipflop. These are the building blocks of registers. As can be seen in the picture above every flipflop is clocked by the same signal. We will also often use the term accumulator. Accumulators are registers which are source and destination registers at the same time. So e.g. on addition they store one input value and after the operation, holding the result in the same register.\n   "},{"id":9,"href":"/docs/digital_logic/05_alu/","title":"05_ALU","section":"Digital Logic","content":"The ALU, the swiss knife of every cpu Today the author wants to introduce an important part of every cpu: The arithmetic logic unit (ALU), we already know about some functions presented in the blog posts before. In this post we want to close the gaps systematically. Time for a short recap: The logic functions \u0026#39;and\u0026#39;,\u0026#39;or\u0026#39;,\u0026#39;not\u0026#39; and \u0026#39;xor\u0026#39; were introduced, also the arithmetic functions \u0026#39;add\u0026#39; and \u0026#39;sub\u0026#39; for integers were shown. Now we want to build most of those functionalities in one unit, the 1-bit alu cell.\n \n     Function description Ainvert Binvert Operation     and\n a \u0026amp; b\n 0\n 0\n 00\n   or\n a | b\n 0\n 0\n 01\n   add\n a + b\n 0\n 0\n 10\n   sub\n a - b\n 0\n 1\n 10\n   slt\n a \u0026lt; b\n 0\n 1\n 11\n   nor\n a nor b\n 1\n 1\n 00\n   nand\n a nand b\n 1\n 1\n 01\n    A 1-bit ALU cell To keep things simple we implement an ALU for 1 bit, which then can be adapted to every bitwidth simply by repitition. We introduce an implementation designed by Hennessey and Peterson, quoted from the popular book Computer Organization and Design (RISC-V) Edition by Hennessey and Patterson.\n \n The implementation will support the fundamental operations \u0026#39;and\u0026#39;,\u0026#39;or\u0026#39;, \u0026#39;add\u0026#39; and \u0026#39;sub\u0026#39;. As we already learned the subtraction can be realised two’s complement: Inverting the input B and setting the carry-in to one.\n  Construct an n-bit ALU The 1bit ALU-cell we created in the first section, can be chained to an n-bit width-alu. The last ALU-cell in the chain, differs a bit from the regular cell as it also includes an overflow-detection circuit. We will get to it later in the article.\n \n \n  Set-less-than We want the complete ALU to support another fundamental instruction set-less-than (slt), necessary to allow branch-operations later on. For this operation the addional input \u0026#39;less\u0026#39; is designed. So the ALU supports an instruction that in C looks like:\n  (a \u0026lt; b)? 1 : 0\n   This can be simply implemented by subtracting b from a, and testing if the value is less than zero. In the implementation, every alu-cell in the chain but the least-significant-bit, gets a zero on the \u0026#39;less\u0026#39; input. The first alu-cell gets the result of the last one, which is representing the most-significant-bit. As the msb also represents the sign bit, we can simply route as input to the least-significant and we are done. This is - however - not true, in case the subtraction might result in an overflow.\n Also, we add a nor-gate with inputs from all result bits to detect zero.\n \n Overflow Detection The last 1-bit ALU cell in the chain has another output \u0026#39;overflow\u0026#39;, to indicate an overflow of the addition of two integer values. The encourages the inclined reader to derive the truth table and circuit as an exercise.\n \n For a two’s complement interpretation overflow occurs in two cases:\n  Two positive numbers are added, the result becomes negative\n  Two negative number are added, the result becomes positive\n   The truth table for overflow is as follows (original source can be found here)\n     Binv a(n-1) b(n-1) c(n-1) OF     0\n 0\n 0\n 0\n 0\n   0\n 0\n 0\n 1\n 1\n   0\n 0\n 1\n 0\n 0\n   0\n 0\n 1\n 1\n 0\n   0\n 1\n 0\n 0\n 0\n   0\n 1\n 0\n 1\n 0\n   0\n 1\n 1\n 0\n 1\n   0\n 1\n 1\n 1\n 0\n   1\n 0\n 0\n 0\n 0\n   1\n 0\n 0\n 1\n 0\n   1\n 0\n 1\n 0\n 0\n   1\n 0\n 1\n 1\n 1\n   1\n 1\n 0\n 0\n 1\n   1\n 1\n 0\n 1\n 0\n   1\n 1\n 1\n 0\n 0\n   1\n 1\n 1\n 1\n 0\n    The circuit generated by Logisim is accordingly:\n \n   Multiplexer \u0026amp; Demultiplexer To select one line out of multiple sources we need another key component, the multiplexer. We can find multiplexer in multiple places, however in an ALU it used to select one of the logic or arithmetic operations, we will see that later.\n \n \n The demultiplexer- as the name suggest- does the exact opposite task and distributes signal on one line (input) to one of multiple outputs, depending on the selection bit(s).\n \n  Multiplexer in complementary Pass-Transistor Logic We show and implement the multiplexer in pass-transistor-logic, as this is the most resource-efficient design…​\n \n    "},{"id":10,"href":"/docs/digital_logic/04_signs/","title":"04_Signs","section":"Digital Logic","content":"Extending the binary system While in the previous blog post about addition, the binary numbers had only one interpretation, we extend the system here to include negative (integer) numbers.\n \n The simplest approach we can think of is to use the most significant bit (MSB) as sign bit, where \u0026#39;0\u0026#39; ist intepreted as sign \u0026#39;+\u0026#39; and \u0026#39;1\u0026#39; is interpreted as \u0026#39;-\u0026#39;. However as we see in the following calculation this does not work as expected:\n \n One complement As the previous approach does not fullfill the requirements let us introduce the one complement and two complement here. The one complement is just an inversion of every bit, independent from its significance.\n \\[ \\begin{array}{l} 00000011_{2} = +3_{10} \\\\ 11111100_{2} = -3_{10} \\end{array} \\]\n  Two complement However,as can be seen in the panel below, there is still a mismatch on addition. So, as a second step the inverted number is incremented by one. This leads us to the so-called two’s complement as seen below.\n \\[ \\begin{array}{l} 00000011_{2} = +3_{10} \\\\ 11111101_{2} = -3_{10} \\end{array} \\]\n As can be seen in the following calculations, with the two’s complement we get the correct results. \n  Overflow As can be seen, in both cases, for one- and two-complement an arithmetic overflow is produced. It is very dependent on the cpu achitecture how those are handled, but in every case you get the information as a flag (v) .\n  Implementation of subtraction in a fulladder To extend the full-adder with the logic for subtraction we do not need to design from scratch all again. The properties of the xor-gate allows the first step of the two’s complement, the inversion of every bit, while for the second step the increment, we simply use \u0026#39;1\u0026#39; of the subtraction switch as carry-input for the first full-adder stage.\n \n  Overview table for the number range -7..+7     decimal\n binary\n one complement\n two complement\n   +7\n 0111\n 0111\n 0111\n   +6\n 0110\n 0110\n 0110\n   +5\n 0101\n 0101\n 0101\n   +4\n 0100\n 0100\n 0100\n   +3\n 0011\n 0011\n 0011\n   +2\n 0010\n 0010\n 0010\n   +1\n 0001\n 0001\n 0001\n   +0\n 0000\n 0000\n 0000\n   -0\n 1000\n 1111\n -\n   -1\n 1001\n 1110\n 1111\n   -2\n 1010\n 1101\n 1110\n   -3\n 1011\n 1100\n 1101\n   -4\n 1100\n 1011\n 1100\n   -5\n 1101\n 1010\n 1011\n   -6\n 1110\n 1001\n 1010\n   -7\n 1111\n 1000\n 1001\n       "},{"id":11,"href":"/docs/digital_logic/03_binary_system/","title":"03_binary_system","section":"Digital Logic","content":"The Binary System All computer we act with on a daily base do not know about the decimal system we are using, based on the ten digits at our hands. All they know about are the states on and off. We build an imaginary circuit with a power source a control light - here an LED (Light Emitting Diode) - and a switch, closing the connection let the light flash. An off light counts as zero, and on light counts as one. Simple as it.We switch the light on and get a one.\n \n Now we extend this installation with a second light and switch left to current one. We switch the left light on and the right on. What we get as a result is a \u0026#39;2\u0026#39;. Switching the right light on again gets us to 3. Like in our decimal system, the significance of the left light is higher (at least in our example), only the factor differs, instead of 10 the factor is 2. We denote the significance with 2^n where n is the position of the light. Every switch is a bit - more exactly the switch is the input and the LED is the output. So with four switches -half a byte - we can count from 0 to 15, while with eight switches - a byte - we can count from 0 to 255. This scheme can be extended as needed.\n     Binary\n Hex\n unsigned Interpretation\n   0000\n 00\n 0\n   0001\n 01\n 1\n   0010\n 02\n 2\n   0011\n 03\n 3\n   0100\n 04\n 4\n   0101\n 05\n 5\n   0110\n 06\n 6\n   0111\n 07\n 7\n   1000\n 08\n 8\n   1001\n 09\n 9\n   1010\n 0A\n 10\n   1011\n 0B\n 11\n   1100\n 0C\n 12\n   1101\n 0D\n 13\n   1110\n 0E\n 14\n   1111\n 0F\n 15\n    \n   A simple Ripple-Carry adder Let us do some simple calculations with the goal to derive the necessary logic for an adder-unit. The addition is done like learned in elementary school, just that this time we add binary numbers. The first example works while the second one producesa carry flag besides the (wrong) result.\n \n   Scheme for combinatorial circuit We develop the RC-adder circuit according to the following scheme, applied to evaluate combinatorial circuits\n  Define inputs and outputs\n  Construct truth table\n  Evaluate boolean equations / simplify\n  Draw optimized combinatorial circuit\n       Truth table for fulladder cell fulladder cell             c_in A B  c_out sum     0\n 0\n 0\n  0\n 0\n   0\n 0\n 1\n  0\n 1\n   0\n 1\n 0\n  0\n 1\n   0\n 1\n 1\n  1\n 0\n   1\n 0\n 0\n  0\n 1\n   1\n 0\n 1\n  1\n 0\n   1\n 1\n 0\n  1\n 0\n   1\n 1\n 1\n  1\n 1\n    \n    \\[ \\begin{aligned} s \u0026amp; = (\\overline{c_{in}} \\land \\overline{A} \\land B) \\lor (\\overline{c_{in}} \\land A \\land {\\overline{B}}) \\lor (c_{in} \\land \\overline{A} \\land \\overline{B}) \\lor (c_{in} \\land A \\land B) \\\\ \u0026amp; = \\overline{c_{in}}(\\overline{A} \\land \\overline{B}) \\lor (A \\land \\overline{B}) \\lor c_{in}\\overline{A} \\land \\overline{B}) \\lor (A \\land B \\\\ \u0026amp; = \\overline{c_{in}}(A \\oplus B) \\lor c_{in}(\\overline{A \\oplus B}) \\\\ \u0026amp; = A \\oplus B \\oplus c_{in} \\end{aligned} \\]\n \\[ \\begin{aligned} c_{out} \u0026amp; = \\overline{c_{in}}(A \\land B) \\lor c_{in}(\\overline{A} \\land B) \\lor c_{in}(A \\land \\overline{B}) \\lor c_{in}(A \\land B) \\\\ \u0026amp; = \\overline{c_{in}}(\\overline{A} \\land \\overline{B}) \\lor (A \\land \\overline{B}) \\lor c_{in}\\overline{A} \\land \\overline{B}) \\lor (A \\land B \\\\ \u0026amp; = \\overline{c_{in}}(A \\land B) \\lor c_{in}[(\\overline{A} \\land B) \\lor (A\\land \\overline{B}) \\lor A \\land B] \\\\ \u0026amp; = \\overline{c_{in}}AB \\lor c_{in}(A \\oplus B) \\lor c_{in}AB \\\\ \u0026amp; = (\\overline{c_{in}} \\lor c_{in})AB \\lor c_{in}(A\\oplus B) \\\\ \u0026amp; = AB \\lor c_{in}A \\oplus B \\end{aligned} \\]\n \n A simpler approach Instead of the circuit of a fulladder cell, by only considering the both input signals without the carry, we evaluate the half-adder cell.\n     A B  c_out sum     0\n 0\n  0\n 0\n   0\n 1\n  0\n 1\n   1\n 0\n  0\n 1\n   1\n 1\n  1\n 0\n    As we can see, the halfadder consists only of the two gates \u0026#39;AND\u0026#39; and \u0026#39;XOR\u0026#39;. Two halfadder and a separate \u0026#39;OR\u0026#39;-gate for the carry-signal result in a fulladder.\n \\[ \\begin{array}{c} c = x \\land y \\\\ s = x \\oplus y \\end{array} \\]\n  \n   Carry-Lookahead Adder To avoid the long delay for the carry signal in the rc-adder, the carry-Lookahead has been developed. The signals, (g)enerate and (p)ropagate are defined as follows (i being the index of the significance):\n \\[ \\begin{array}{c} g_{i} = a_{i} \\land b_{i} \\\\ p_{i} = a_{i} \\lor b_{i} \\end{array} \\]\n From these helper signals the next carry-value is calculated:\n \\[ c_{i+1} = g_{i} \\lor c_{i} \\land p_{i} \\]\n \\[ \\begin{aligned} c_{1} \u0026amp; = g_{0} \\lor c_{0}p_{0} \\\\ c_{2} \u0026amp; = g_{1} \\lor (g_{0} \\lor c_{0}p_{0})p_{1} = g_{1} \\lor g_{0}p_{1} \\lor c_{0}p_{0})p_{1} \\\\ c_{3} \u0026amp; = g_{2} \\lor c_{2}p_{2} = g_{2} \\lor (g_{1} \\lor g_{0}p_{1} \\lor c_{0}p_{0}p_{1})p_{2} \\\\ \u0026amp; = g_{2} \\lor g_{1}p_{2} \\lor g_{0}p_{1}p_{2} \\lor c_{0}p_{0}p_{1}p_{2} \\\\ c_{4} \u0026amp; = g_{3} \\lor c_{3}p_{3} = g_{3} \\lor (g_{2} \\lor g_{1}p_{2} \\lor g_{0}p_{1}p_{2} \\lor c_{0}p_{0}p_{1}p_{2})p_{3} \\\\ \u0026amp; = g_{3} \\lor g_{2}p_{3} \\lor g_{1}p_{2}p_{3} \\lor g_{0}p_{1}p_{2}p_{3} \\lor c_{0}p_{0}p_{1}p_{2}p_{3} \\\\ \\end{aligned} \\]\n Carry-Lookahead Adder circuit\n \n As can be seen the circuit complexity increases with the significance. The table below shows the total view of these different adder implementations. Of course the topic of adders is much broader as displayed here, we only introduced the concepts.\n Ripple-Carry Adder\n \n Carry-Lookahead Adder\n \n In the next blog post we will see, how to extend the range of numbers to the negative space.\n   "},{"id":12,"href":"/docs/digital_logic/02_xor/","title":"02_XOR","section":"Digital Logic","content":"XOR As there are a lot of important applications for the exclusive-or (XOR) operation it is dedicated an own blog post. Two of the most important are:\n   XOR operations are often used in a Linear Feedback Shift Registers (LFSR). LFSR are a crucial step in scrambler / descrambler respectively, which are used in transmitters / receivers\n  Part of an encryption algorithmus\n  Last but not least an important part of a halfadder.\n   The exclusive-or operation xor which is also termed as antivalence is denoted as A ⊕ B = Q. Its truth table is shown below\n     A\n B\n Q\n   0\n 0\n 0\n   0\n 1\n 1\n   1\n 0\n 1\n   1\n 1\n 0\n    \n     CMOS XOR\n   \n         Some Applications Linear Feedback Shift Register (LSFR) As described above XOR gates are used in Linear Feedback Shift Registers (LFSR). LFSR are often used for test pattern generation. The author selected a simple example of a 3-bit LFSR found in a white paper from Texas Instruments.\n \n For this example we need to take a big leap and enter the area of sequential circuits. These are circuits which make use of feedback. The chosen example generates pseudo-random test patterns according to the table below. The registers are feed with the seed value 111, after 8 clock cycles the patterns repeat again. Of course real test pattern generators have a much higher bit width.\n     CLK\n FF1\n FF2\n FF3\n   0\n 1\n 1\n 1\n   1\n 0\n 1\n 1\n   2\n 0\n 0\n 1\n   3\n 1\n 0\n 0\n   4\n 1\n 0\n 0\n   5\n 0\n 1\n 0\n   6\n 1\n 0\n 0\n   7\n 1\n 1\n 0\n   8\n 1\n 1\n 1\n     Halfadder In the next blogpost we dive deeper into the halfadder application. The halfadder essentialy consists of the two following particular equations for sum (making use of the XOR ) and carry (using a simple AND).\n \\[ \\begin{array}{c} c = x \\land y \\\\ s = x \\oplus y \\end{array} \\]\n    "},{"id":13,"href":"/docs/digital_logic/01_boolean_algebra/","title":"01_boolean_algebra","section":"Digital Logic","content":"Boolean Algebra and Basic Logic Gates We are starting (the journey) with three very basic logic functions (or operations), which despite their simplicity already are valid instructions of a regular CPU:\n NOT, AND and OR\n Logical functions - also called operations, the author will use both terms interchangebly here - can be described in various ways, most often it is described implicit as boolean equation, but it can also be explained explicit in a truth table.\n A truth table is a table with the a column for every input and a column for every output. So it shows the output respective the outputs of the function for all possible combinations of inputs. Also they can contain columns with intermediate values.\n The truth tables shown in this lesson are in general very small and show only combinations of two inputs, although all the functions joining inputs can be expanded to a arbitrary number of inputs.\n Boolean equations can be transformed to truth tables and vice versa. Later, we will also introduce Karnaugh maps, a way to optimize boolean logic in a graphical way.\n NOT The not operation also termed as inversion, is denoted as \\[\\overline{A} = \\neg A = Q\\]\n     A\n Q\n   0\n 1\n   1\n 0\n    \n The not operation is the only one with only one input, it can not be expanded.\n Accordingly the gate-level implementation is also known by the name inverter. Despite its seemingly simple functionality there is a lot to say about the inverter for both the logical as well the implementation, so the author will dedicate it an own post in the mid-feature.\n  AND The and operation also termed as conjunction is denoted as \\[A \\land B = Q\\]\n     A\n B\n Q\n   0\n 0\n 0\n   0\n 1\n 0\n   1\n 0\n 0\n   1\n 1\n 1\n    \n  OR The or operation also termed as disjunction is denoted as \\[A \\lor B = Q\\]\n     A\n B\n Q\n   0\n 0\n 0\n   0\n 1\n 1\n   1\n 0\n 1\n   1\n 1\n 1\n    \n  Although boolean algebra on itself is an interesting field, we will only skim the subject briefly here, just enough to get a feeling and understanding how to work with boolean formulas.\n   Note  If you are interested in a more profound view, providing induction and proof, I refer you to accordingly literature (sources listed at the end).     The laws of the boolean algebra are shown in the following table:\n     Law ∧ (conjunction, AND operator) ∨ (disjunction, OR operator)     commutative law\n \\[p \\land q = q \\land p\\]\n \\[p \\lor q = q \\lor p\\]\n   associative law\n \\[p \\land (q \\land r) = (p \\land q) \\land r = pqr\\]\n \\[p \\lor (q \\lor r) = (p \\lor q) \\lor r = p \\lor q \\lor r\\]\n   absorptions law\n \\[p \\land (p \\lor q) = p\\]\n \\[p \\lor (p \\land q) = p\\]\n   distributive law\n \\[p \\lor (q \\lor r) = (p \\land q) \\lor (p \\land r) = pq \\lor pr\\]\n \\[p \\lor (q \\lor r) = (p \\lor q) \\lor (p \\lor r) = (p \\lor q)(p \\lor r)\\]\n   neutral elements\n \\[p \\land 1 = p\\]\n \\[p \\lor 0 = p\\]\n   complem. element\n \\[p \\land \\neg p = 0\\]\n \\[p \\lor \\neg p = 1\\]\n   Reference\n Hans-Jochen Bartsch\n Taschenbuch Mathematischer Formeln, 20. Auflage, p. 27- 28\n    You probably know intuitively - or from school- the first two laws mentioned in the table, the commutative law and the associative laws. As you can see, every law can be applied to conjunctions as well as disjunctions without any exceptions.\n The commutative law implies that the order of the variables is neutral for the operation and can be swapped without changing the result of the operation.\n The associative law implies, that parenthesis are swappable. Conjunctions can be condensed without operator.\n The absorptions law is probably not known to you from school, as it is only used in logic and has no counterpart in at least school mathmatics.\n The distributive law, again, is known from school. It implies that variables / operations outside of paranthesis needs to be applied to all variables in the parenthesis.\n The law of neutral elements, again is something special to boolean algebra. The logic one is the correspondent to \u0026#39;true\u0026#39;, and so p and true equals \u0026#39;true\u0026#39;, so as the logic zero is \u0026#39;false\u0026#39; and so p or false equals p.\n The complementary law implies, that to every element (variable) p there is a complementary element (variable) p, so that the conjunction results in a logical zero while disjunction results in a logical one.\n  At this point the author has to leap ahead a bit as some practical issues on the electrical level need some thought on their logic level: On the implementation level (for an electrical implementation) it is rather uncommon to use AND,OR and NOT directly, instead the inverse functions NAND and NOR are used.\n  NAND and NOR The NAND operation is denoted as \\[\\overline{A \\land B} = Q\\] while the NOR operation is denoted as \\[\\overline{A \\lor B} = Q\\]\n     NAND\n NOR\n   \n \n    As you can see, for those two functions the results are exactly the inverse of their respective complement (NAND ⇐⇒ AND,NOR ⇐⇒ OR), in that sense the author recommends the construction of the respective truth tables as an exercise to the reader.\n    The De Morgan theorem In addition to those merely basic axioms introduced above, there is the De Morgan theorem, which we need to easily convert between NAND and NOR. For the sake of simplicity we only show for two elements however the theorem is independent from any number of elements / inputs.\n \\[\\overline{p_1 \\land p_2} = \\overline{p_1} \\lor \\overline{p_2}\\]\n     A\n B\n \\[\\overline{A}\\]\n \\[\\overline{B}\\]\n \\[\\overline{AB}\\]\n \\[\\overline{A} \\lor \\overline{B}\\]\n   0\n 0\n 1\n 1\n 1\n 1\n   0\n 1\n 1\n 0\n 1\n 1\n   1\n 0\n 0\n 1\n 1\n 1\n   1\n 1\n 0\n 0\n 0\n 0\n    \n \\[\\overline{p_1 \\lor p_2} = \\overline{p_1} \\land \\overline{p_2}\\]\n     A\n B\n \\[\\overline{A}\\]\n \\[\\overline{B}\\]\n \\[\\overline{AB}\\]\n \\[\\overline{A} \\lor \\overline{B}\\]\n   0\n 0\n 1\n 1\n 1\n 1\n   0\n 1\n 1\n 0\n 0\n 0\n   1\n 0\n 0\n 1\n 0\n 0\n   1\n 1\n 0\n 0\n 0\n 0\n    \n Now we are finally equipped to continue with the electrical part / description.\n   Implementation on electrical level First we are introducing some common electronic components and their symbols used in electric circuit schematics.\n \n The diode We are beginning the journey with a simplificated circuit design for \u0026#39;and\u0026#39; and \u0026#39;or\u0026#39; called wired logic. This circuit design is so simple it is even not possible to implement an inverter in it.\n     wired and\n wired or\n   \n \n        As you can see, this circuit is not to complicate - even laymans in electronc should be able to identify the essential parts of - the inputs are connected to a diode, each. A diode is a simple semiconductor which acts as a one-way for electric current. Semiconductors are a group of materials (only silicon and germanium are useful here, due to their chemical properties), not really conductor but also no insulator. In fact the conductiviy of the material is dependent on the deliberated pollution of their crystall lattice structure with elements of the fifth main group, for an n(egative)-dotted material respectively of the third main group for a p(ositive) dotted material. This process is called dotting. For the interested reader here is a link to all the physical background the author was to lazy to repeat since it is often and better explained on the internet already ;-) .\n \n All we need to know right know is that a diode consist of one substrate (in most cases today silicon) which becomes n-dotted on one side and p-dotted on the other, forming a pn-junction in between acting as said one-way barrier: It is possible for electrons to rush from n-side to the p-side but not the other way round.\n The second component of the circuits shown above is a resistor, its solely purpose is to reduce the current flowing. For wire-and it is wired as pull-up resistor while for wire-or it is wired as pull down resistor. The wire-and only reaches a sufficient high-level if all inputs go high-level. Similar the wire-or only goes low-level if neither of the inputs goes high level.To prevent current flowing from pull-up resistor to the output of the previous circuit (wire and) respectively current flowing from one input back to the other input (wire-or), the diodes are in place.\n A problem of this setup which we will definitely encounter at some point, is that the signal is weakend when flowing from input to the output and there is no ability provided to recover the signal, so at the output the signal level might not be distinguished correctly by the subsequent circuit. Allow the author a remark in a subtle detail in the terminology at this point: Although it is often described as amplifying we want holding on here, that we want a somewhat \u0026#39;intelligent\u0026#39; signal amplifier here recognizing the signal level of the input signal and recovering, complementary to a \u0026#39;stupid\u0026#39; amplifier just amp-ing the input signal.\n So the diodes are a fine component, e.g. useful when protecting parts of the circuit from electrostatic discharge (ESD), but for our logic it is not sufficient.\n What if we had a component capable to amplify the signal, so we could design circuits also recovering the signal levels with it? Luckily such a component exists and is introduced in the next section.\n  The MOSFET transistor The type of transistor we want to introduce and use here is a MOSFET (Metal Oxid Semiconductor Field Effect Transistor) - as opposed to classical bipolar transistor. As the name suggests a MOSFET is a transistor (or semiconductor) where the load currrent can be controlled by the strength of the electrical field, created on the gate input\n \n The image above sketches the principal structure of a mosfet on the silicon. Source and drain are both connected to an own n-well, in the p dotted substrate, while the gate in between is isolated by a thin silicon dioxide layer. There is a forth connector \u0026#39;bulk\u0026#39;, leading to the substrate, in discrete MOSFETs connected to the source, but for now this one is not relevant. There is a lot of complex physics behind the workings of a MOSFET all we want to know for now, is that when a voltage is applied to the gate, an electrical field is induced, which creates a chanel between source and drain and allows electrons to flow from source to drain. The higher the gate voltage the bigger (wider) the channel, the more electrons flowing (until a certain boundary of course). And if the gate voltage is zero, also the channel is non-existent.\n \n Common used symbols for MOSFET used in electrical schematics as well as some other we need due to course are shown below, some has more then only one, especially the MOSFET has a number of sligthly various symbols reflecting the differences in the physical component.\n \n With the MOSFET element introduced and a single resistor,an element which reduces the current flow, added, we can straight-forward implement an inverter circuit as shown in the image above. Source is connected to the ground and Drain is connected to the inverter output and also to the voltage source via a high-impedance resitor. Gate is the input of the inverter. When the voltage is set to low on the gate, the MOSFET does not conduct and so the potential available on the output is sufficient for a logic one. Whene a voltage is set to the gate,the drain-source path of the MOSFET becomes conductive, the potential available on the output breaks down an drains away via the drain-source path.\n \n  On the electrical level inverter has two different tasks to fullfill\n  Refreshing the signal (level) the inverters gets from the preceeding circuit\n  Actually, inverting the signal\n   There are also integrated circuits, called buffer or driver, solely dedicated to the first task mentioned, we will discuss that in a later blog post / lesson. Here we want to focus only on the logical part of inverting the signal, although for that both functions are equal important, thats why they are combined in one circuit. Of course, the logic signals have to be refreshed also in other logical circuits - like and and or - but in most cases this task is solely handeled by those inverter stages, afterwards or before.\n \n In the symbol the first task, the refreshing of the signal (level), is indicated by the triangle, while the little circle denotes the actual inverting function.\n      NMOS NAND\n NMOS NOR\n   \n \n        Now it becomes clear, why the inverse function of AND and OR on the gate level is simpler than the original function. Because you have to add an inverter circuit afterwards, increasing the transistor count.\n One big drawback of the implementation is the high power consumption caused by the pull-up resistor. Let us see if there is a way to solve that issue. What if there is transistor labeled PMOS with a complementary structure - p-wells on an n dotted substrate - to our up-to-now used NMOS circuit?\n When we use those instead the resistor for the pull-up path we can drastically reduce power dissipation. As you guess this technique, both types of transistor combined, exists and is named CMOS ( Complementary Metal Oxid Semiconductor).\n     CMOS NAND\n CMOS NOR\n   \n \n        It becomes apparent now where the CMOS technology has its name from: As we can see in the implementation of NAND and NOR in CMOS technology, the P circuit above is exactly the complementary of the N circuit below. For the inverter this property was just not recognizable due to the symmetry. But we can also see the drawback of the CMOS technology: The number of transistors doubles, increasing the effort and complexity of the manuacturing process (complexer masks and layouts, more processing steps due to different types of transistors, nmos and pmos both on one wafer) and the integrated circuit as the end product.\n However the toolchain, the CMOS process and its related technologies are evolved and matured already since decades, so today it does not matter anymore, in fact recently Google even published the SkyWater Open Source PDK 130, a so-called Process Design Kit offering electrical engineers a tool(chain) to produce designs for Application Specific Integrated Circuits which then can directly be manufactured using a 130nm process. You can read more about it here on Hackaday.\n That was a first slight look into boolean logic and its implementation on silicon, of course there is a lot more to it than only one reference design, also we did not had a look yet for ESD measurements and protection circuits for the inputs and output.\n In the next post we have a look on the exclusive-or (XOR) operation.\n    "},{"id":14,"href":"/pages/overview/","title":"How does a CPU work? Overview","section":"Pages","content":"Introduction and overview Introduction and overview\n Every topic of this course is broken up into two parts: A part about the mathematical side of digital logic, ideally almost independent from implementation details and a more concrete part handling this very details. In the concrete part we will mostly discuss the implementation on an electrical layer, though there are still other solutions possible based on fluid dynamics, or optical gates, most computers and logic gates are still based on electronics and semiconductor technology and this will not change with quantum computing, which is mostly tied to / adjunct to probability theory. However with quantum computing the fundamentals behind which the author describes here will completly change.\n   Combinatorial Logic Introduction to Combinatorial Logic\n Short introduction to combinatorial logic,and its various description models.\n   A first look at digital logic Boolean Algebra, basic logic gates and their implementation\n In this lesson we will discover truth tables, boolean notation and learn about De Morgan’s laws. Also we have a look at the most basic logic gates and why their are usually are not implemented as such in electronics, having a look at NAND and NOR instead.\n   You have to choose! A complex gate: XOR\n Another CPU instrucion and as a side-note and preparation for the next topic, we have a look at the XOR gate. As in the first lesson we’ll see first the logic part and then its translation into an electrical implementation.\n   01+01 = 10 Binary systems (Part I) and Combinatorial Logic (Part I)\n Here we explain the basic concept of how to count with only two different states at hand. Also we will learn how to sum up two positive integer values in the binary system, derivate the logic needed for a full-adder, to dive into the topic of combinatorial logic.\n   There is plenty room in the negative space Binary Systems (Part II) signed integer\n Second part about the binary system. In this post we learn about one- and two- complement and subtraction.\n   Adding is not all Combinatorial Logic (Part II), a simple ALU\n We want close the first series by this second chapter about combinatorial logic. We will extend the adder to a simple ALU\n   Outline, what comes next From combinatorial to sequential\n In this first course we learned some basics about digital logic. But all these examples use only static logic, we are still lacking a concept of how to store computed values. We need to learn the concept of registers and sequential logic. This will be the main topic of the second post series.\n   Extra Let me look that up First look into Memory and Look-Up-Tables (LUTs)\n In this lesson we learn, that - in principal - every form of combinatorial logic can be transformed into \u0026#39;software\u0026#39; by putting it into memory. In a practical example we will show how a DDS sine wave generator use a Look-Up-Table (LUT) to replicate the sine.\n   "},{"id":15,"href":"/pages/about/","title":"About","section":"Pages","content":"A simple static blog meant for writing about topics like electronics \u0026amp; computation and - maybe - philosophy.\n  Dear reader of my humble blog , I am Sven Wehrend, a scanner personality born 1987, interested in a wide range of different topics reaching from technology over psychology to spirituality.\n I studied Computer Engineering at the HTW Berlin (Bachelor) and TU Berlin (Master).\n  "},{"id":16,"href":"/posts/web/00_setting_up_nikola/","title":"Setting up a static blog with Nikola","section":"Web","content":"I picked up Nikola some months ago as I was searching for a simple static-site-generator to setup a blog with minimal effort.\n To make things a bit more interesting and at the same time easier we will use Nikola in a Docker container. So I require an almost basic understanding for Docker here.\n First create a directory on our host system, where you want the files stored, e.g. \u0026#39;blog\u0026#39; in your home directory.\n You can pull unoffical images with the most recent version via:\n docker pull dragas/nikola   and run the image getting a (bash) shell via:\n docker run -it --network=\u0026#34;host\u0026#34; -v ~/blog:/nikola dragas/nikola   here the option flag -it stands for interactive - which, in fact, it does not but it is easy to memorize so, so let it be so. Furthermore we need to add our volume, a storage resource which is shared between the host system and the docker container. So we add -v ~/blog for our blog located in our home directory (for linux, on windows systems the paths differ). Also we need to add the option --network=\u0026#34;host\u0026#34; otherwise we won’t be able to access the webpage from our host system, and thus from our browser. If you are interested, check Docker run\n Here you can execute the common bash commands like cd, ls, ps and so on, feel free to test and explore Basic bash commands :-)\n Now that you have explored the system a bit, you can initiate a basic nikola project via:\n nikola init   You will be led through some questions regarding your blog, for destination simply set .\n We create a first post with:\n nikola new_page -t \u0026#34;Hello World\u0026#34;   Write the post on your host system on an editor of your choice it is located in blog/posts/hello-world.rst and then build via:\n nikola build   To test the website, start the test server with:\n nikola serve   and head your browser to http://localhost:8080 .\n \n This is cool, but surely we want do a bit of styling and customization now, as the current look is pretty standard. The author’s preference is a plain and simple style without much unnecessary payload.So I searched for such theme, but they were not to my liking. Then I found the Nikola port of Hyde\n The theme can be installed with:\n nikola theme -i hyde   After installation the theme needed some tweaking of the about section in the sidebar. So the version shown here diverges from the original look. Adjustments were made to the file assets/css/hyde.css in the directory of the freshly installed theme. Ideally the changes should be made in an additional file custom.css\n \n Much better now!\n If we are satified with the result, we can deploy our new static site to web space of our choice. For this we adjust the deploy setting in th conf.py file. This can look like the following.\n DEPLOY_COMMANDS = { \u0026#39;default\u0026#39;: [ \u0026#39;rsync -rav --delete output/ user@server:/var/www/virtual/user/html\u0026#39;, \u0026#39;rdiff-backup output ~/blog-backup\u0026#39;, ] }   Now, we can simply execute this command by typing\n nikola deploy   and the new site is part of the www.\n "},{"id":17,"href":"/posts/web/01_writing-about-math-with-asciidoc/","title":"Writing about math with Asciidoc","section":"Web","content":"While drafting a blog post, part of a series of posts intended to be published in the future, I discovered a lack of my tooling in Nikola. When adding mathematical formulas, I realized that there is no default-way of rendering images from those math formulas, and inserting them as image-tags into the html. Instead math formulas are handled via JS, which I try to reduce to an absolute minimum on my site. So I started to implement a rather quick-and-dirty plugin to do exactly this transformation, using latex and dvipng or dvisvgm. Then I come across this blog post where reading about asciidoc written in python and its successor asciidoctor written in ruby supporting different display formats. Ideal for the content I had in mind. So I installed the Nikola plugin asciidoc (which is mostly a wrapper to the asciidoc binary called as a subprocess), via\n nikola plugin -i asciidoc   in my local environment and converted my blogpost(s) first semi-automatically which is, due to the metadata-section from Nikola, not the very best idea and continued manually. After some frustrations with the paths for the latex filter inside asciidoc it was working.\n So as a sample of one of the future blogposts here one or rather two math-equations rendered this way:\n \\[ \\begin{array}{c} c = x \\land y \\\\ s = x \\oplus y \\end{array} \\]\n Just as side-note: It describes a halfadder.\n Edit Improved grammar.\n "},{"id":18,"href":"/drafts/alu2/","title":"Alu2","section":"Drafts","content":" title: How does a CPU work? The swiss knife (Part II)\n  slug: how-does-a-cpu-work-alu-2\n  date: 2022-02-09\n  category:\n  link:\n  description:\n  type: text\n   Instruction table     logic operations description implementation data type / size     and\n basic boolean Op\n -\n bit\n   or\n basic boolean Op\n -\n bit\n   not\n basic boolean Op\n -\n bit\n   xor\n complex boolean Op\n -\n bit\n   shl\n shift left\n -\n various\n   shr\n shift right\n -\n various\n        control instructions description data type / size     ld\n load\n various\n   st\n store\n various\n   jx\n jump (x= gt,eq, lt )\n various\n        arithmetic operations description implementation data type / size     ashl\n arithmetic shift left\n -\n various\n   ashr\n arithmetic shift right\n -\n various\n   inc\n increment\n -\n various\n   dec\n decrement\n -\n various\n   add\n addition\n -\n (u) integer\n   sub\n subtraction\n -\n (u) integer\n   mul\n multiplication\n software-routine or HW\n (u)integer\n   div\n division\n software-routine or HW\n (u)integer\n   sin\n sine\n software-routine (Cordic)\n (u) integer\n   cos\n cosine\n software-routine (Cordic)\n (u) integer\n   tan\n tangens\n software-routine (Cordic)\n (u) integer\n     Compare operations The first type of instructions we have nor called not even discussed are compare instructions. We first show a comparator for a single bit, and then derive an comparator for arithmetic operations.This whole section is just a quotation, the original source is found here.\n     A B \u0026gt; (G) = (E) \u0026lt; (L)     0\n 0\n 0\n 1\n 0\n   0\n 1\n 0\n 0\n 1\n   1\n 0\n 1\n 0\n 0\n   1\n 1\n 0\n 1\n 0\n    This leads us to three simple boolean equations (greater than, equal, less than):\n \\[ G = A\\overline{B} \\]\n \\[ E = \\overline {A \\oplus B} \\]\n \\[ L = \\overline{A}B \\]\n \n We extend this scheme to a 4 bit width comparator as follows:\n \\[ A = A_{1}A_{2}A_{3}A_{4} \\quad and \\quad B=B_{1}B_{2}B_{3}B_{4} \\]\n  Greater than \\huge \\[ \\begin{array}{l} (1) \\quad A_{1} \u0026gt; B_{1} ⇒ A \u0026gt; B \\quad or \\quad G=1 \\\\ (2) \\quad A_{1} = B_{1};A_{2} \u0026gt; B_{2} ⇒ A \u0026gt; B \\quad or \\quad G=1 \\\\ (3) \\quad A_{1} = B_{1};A_{2} = B_{2};A_{3} \u0026gt; B_{3} ⇒ A \u0026gt; B \\quad or \\quad G=1 \\\\ (4) \\quad A_{1} = B_{1};A_{2} = B_{2};A_{3} = B_{3};A_{4} \u0026gt; B_{4} ⇒ A \u0026gt; B \\quad or \\quad G=1 \\end{array} \\]\n \\huge \\[ \\begin{array}{l} For \\quad (1) \\quad G = A_{1}\\overline{B}_{1} \\\\ For \\quad (2) \\quad G= \\overline {A_{1} \\oplus B_{1}} (A_{2}\\overline{B_{2}}) \\\\ For \\quad (3) \\quad G= \\overline {A_{1} \\oplus B_{1}} \\quad \\overline {A_{2} \\oplus B_{2}} (A_{3}\\overline{B_{3}}) \\\\ For \\quad (4) \\quad G= \\overline {A_{1} \\oplus B_{1}} \\quad \\overline {A_{2} \\oplus B_{2}} \\quad \\overline {A_{3} \\oplus B_{3}} (A_{4}\\overline{B_{4}}) \\end{array} \\]\n From this follows, that G=1 when either of the above equations holds…​\n \\huge \\[ \\begin{array}{l} G= A_{1}\\overline{B}_{1} + \\overline {A_{1} \\oplus B_{1}} (A_{2}\\overline{B_{2}}) \\overline {A_{1} \\oplus B_{1}} \\quad \\overline {A_{2} \\oplus B_{2}} (A_{3}\\overline{B_{3}}) + \\\\ \\overline {A_{1} \\oplus B_{1}} \\quad \\overline {A_{2} \\oplus B_{2}} \\quad \\overline {A_{3} \\oplus B_{3}} (A_{4}\\overline{B_{4}}) \\end{array} \\]\n   Less than \\huge \\[ \\begin{array}{l} (5) \\quad A_{1} \u0026lt; B_{1} ⇒ A \u0026lt; B \\quad or \\quad L=1 \\\\ (6) \\quad A_{1} = B_{1};A_{2} \u0026lt; B_{2} ⇒ A \u0026lt; B \\quad or \\quad L=1 \\\\ (7) \\quad A_{1} = B_{1};A_{2} = B_{2};A_{3} \u0026lt; B_{3} ⇒ A \u0026lt; B \\quad or \\quad L=1 \\\\ (8) \\quad A_{1} = B_{1};A_{2} = B_{2};A_{3} = B_{3};A_{4} \u0026lt; B_{4} ⇒ A \u0026lt; B \\quad or \\quad L=1 \\end{array} \\]\n \\huge \\[ \\begin{array}{l} For \\quad (5) \\quad L= \\overline{A}_{1}B_{1} \\\\ For \\quad (6) \\quad L= \\overline {A_{1} \\oplus B_{1}} (\\overline{A_{2}}B_{2}) \\\\ For \\quad (7) \\quad L= \\overline {A_{1} \\oplus B_{1}} \\quad \\overline {A_{2} \\oplus B_{2}} (\\overline{A_{3}}B_{3}) \\\\ For \\quad (8) \\quad L= \\overline {A_{1} \\oplus B_{1}} \\quad \\overline {A_{2} \\oplus B_{2}} \\quad \\overline {A_{3} \\oplus B_{3}} (A_{4}\\overline{B_{4}}) \\end{array} \\]\n From this follows, that L=1 when either of the above equations holds…​\n \\huge \\[ \\begin{array}{l} L= \\overline{A}_{1}B_{1} + \\overline {A_{1} \\oplus B_{1}} (\\overline{A_{2}}B_{2}) \\overline {A_{1} \\oplus B_{1}} \\quad \\overline {A_{2} \\oplus B_{2}} (\\overline{A_{3}B_{3}}) + \\\\ \\overline {A_{1} \\oplus B_{1}} \\quad \\overline {A_{2} \\oplus B_{2}} \\quad \\overline {A_{3} \\oplus B_{3}} (\\overline{A_{4}}B_{4}) \\end{array} \\]\n   Equal Last but not least for equal holds:\n \\huge \\[ \\begin{array}{l} A_{1}=B_{1}; A_{2}=B_{2};A_{3}=B_{3};A_{4}=B_{4} ⇒ E=1 \\\\ E = \\overline {A_{1} \\oplus B_{1}} \\quad \\overline {A_{2} \\oplus B_{2}} \\quad \\overline {A_{3} \\oplus B_{3}} \\quad \\overline {A_{4} \\oplus B_{4}} \\end{array} \\]\n Thus, the logical circuit is designed as follows:\n \n Comparator circuit The 4063 cmos IC is a 4 bit comparator IC. It can be cascaded to cover wider bit ranges.\n     Shift operations The next important set of operations are the shift operations. Those can be divided in logical as well as arithmetic shift operations.\n \n \n As you may have noticed, in the last posts we have not even mentioned the two more advanced fundamental arithmetic operations multiplication and divison of integers. mul and div are very elaborate operations compared to addition and subtraction.\n Simple CPUs and microprocessors do not even have multiplier units or division units. The instructions have to be programmed as a software routine, we go into this in more details in another blogpost. (And then there is of course also floating point arithmetic, even more complex than our currently discussed integers).\n The now introduced arithmetic shift operations solves multiplication and division operations at least for a subset of powers of two:\n  An arithmetic left shift of a two’s complement value by n bits equals a multiplication by 2n. (Given no overflow is produced)\n    An arithmetic right shift equals the floor of a division by 2n.\n   A simple Shifter The gate-level implementation of a simple shifter is shown below.\n \n Next we see the truth table for the decoder logic, the derivation of the netlist is left as excercise for the reader.\n     Sel1 Sel0 R nop L     0\n 0\n 0\n 1\n 0\n   0\n 1\n 0\n 0\n 1\n   1\n 0\n 1\n 0\n 0\n    We see the gate-level implementation of such a shifter- is shown for the operations is realised in pass-transistor-logic (Reference: VLSI Design by K.Lal Kishore and V.S.V Prabhakar).\n \n  Barrel Shifter A more sophisticated shifter implementation is the so known barrel shifter. The barrel shifter allows a shift over multiple bits in one go.\n An implementation in pass-transistor-logic is shown below.\n \n    "}]